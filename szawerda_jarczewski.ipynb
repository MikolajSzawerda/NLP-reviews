{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T20:34:29.468771Z",
     "start_time": "2024-06-15T20:34:29.302518Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T19:42:17.354510Z",
     "start_time": "2024-06-15T19:42:17.350313Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        encoding = self.tokenizer(review, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(rating)\n",
    "\n",
    "\n",
    "class ReviewDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_file, batch_size=8, val_split=0.2, num_workers=7):\n",
    "        super().__init__()\n",
    "        self.data_file = data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Load data from CSV file\n",
    "        data = pd.read_csv(self.data_file)\n",
    "        self.reviews = data['review'].tolist()\n",
    "        self.ratings = data['rating'].tolist()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = ReviewDataset(self.reviews, self.ratings)\n",
    "        val_size = int(len(dataset) * self.val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewRatingModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes: int, learning_rate: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.accuracy(logits, labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.accuracy(logits, labels), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | bert       | DistilBertModel    | 66.4 M\n",
      "1 | classifier | Linear             | 3.8 K \n",
      "2 | criterion  | CrossEntropyLoss   | 0     \n",
      "3 | accuracy   | MulticlassAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "66.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.4 M    Total params\n",
      "265.467   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a48326a02e433e8d1abf6ba4bf16d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0763133921a84aadb21f1358513e090a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"logs\", name=\"dev\")\n",
    "\n",
    "dm = ReviewDataModule(\"data/train_data.csv\", batch_size=4, num_workers=16)\n",
    "model = ReviewRatingModel(num_classes=5)\n",
    "trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('data/train_data.csv')\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=1500)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(df['review'])\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "print('Bag of words completed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T20:30:35.095558Z",
     "start_time": "2024-06-15T20:30:33.572489Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6090271424214699\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.59      0.63       242\n",
      "           1       0.44      0.41      0.42       277\n",
      "           2       0.40      0.23      0.29       352\n",
      "           3       0.52      0.50      0.51       961\n",
      "           4       0.70      0.82      0.76      1447\n",
      "\n",
      "    accuracy                           0.61      3279\n",
      "   macro avg       0.55      0.51      0.52      3279\n",
      "weighted avg       0.59      0.61      0.60      3279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = train_data_features\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['rating'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T19:54:10.155668Z",
     "start_time": "2024-06-15T19:54:09.368941Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 44\n",
      "max: 13501\n",
      "avg: 721.2689116642265\n",
      "std: 676.4062576996707\n"
     ]
    }
   ],
   "source": [
    "review_lengths = df['review'].apply(lambda x: len(x))\n",
    "\n",
    "min_length = review_lengths.min()\n",
    "max_length = review_lengths.max()\n",
    "avg_length = review_lengths.mean()\n",
    "stddev_length = review_lengths.std()\n",
    "\n",
    "print(f\"min: {min_length}\")\n",
    "print(f\"max: {max_length}\")\n",
    "print(f\"avg: {avg_length}\")\n",
    "print(f\"std: {stddev_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T19:58:07.835583Z",
     "start_time": "2024-06-15T19:58:07.815689Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "### words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 7\n",
      "max: 1931\n",
      "avg: 103.85474621766716\n",
      "std: 98.8863525280012\n"
     ]
    }
   ],
   "source": [
    "review_lengths = df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "min_length = review_lengths.min()\n",
    "max_length = review_lengths.max()\n",
    "avg_length = review_lengths.mean()\n",
    "stddev_length = review_lengths.std()\n",
    "\n",
    "print(f\"min: {min_length}\")\n",
    "print(f\"max: {max_length}\")\n",
    "print(f\"avg: {avg_length}\")\n",
    "print(f\"std: {stddev_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T19:58:27.623499Z",
     "start_time": "2024-06-15T19:58:27.541493Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag Of Words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BoWModel(L.LightningModule):\n",
    "    def __init__(self, num_classes: int, learning_rate: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1500, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        review, labels = batch\n",
    "        logits = self(review)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.accuracy(logits, labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        review, labels = batch\n",
    "        logits = self(review)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.accuracy(logits, labels), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T20:35:38.499582Z",
     "start_time": "2024-06-15T20:35:38.491221Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_indices = np.random.rand(len(df)) > 0.2\n",
    "train_data = torch.from_numpy(train_data_features).float()[train_indices]\n",
    "train_targets = torch.from_numpy(df[\"rating\"].values[train_indices]).long()\n",
    "\n",
    "test_data = torch.from_numpy(train_data_features[~train_indices]).float()\n",
    "test_targets = torch.from_numpy(df[\"rating\"].values[~train_indices]).long()\n",
    "train_dataset = TensorDataset(train_data, train_targets)\n",
    "val_dataset = TensorDataset(test_data, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T20:35:41.917594Z",
     "start_time": "2024-06-15T20:35:41.783953Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss   | 0      | train\n",
      "1 | accuracy  | MulticlassAccuracy | 0      | train\n",
      "2 | model     | Sequential         | 935 K  | train\n",
      "---------------------------------------------------------\n",
      "935 K     Trainable params\n",
      "0         Non-trainable params\n",
      "935 K     Total params\n",
      "3.741     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e088c704b88846ed8738807f55bbabb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcinjarczewski/Documents/studia/sem6/NLP-reviews/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/marcinjarczewski/Documents/studia/sem6/NLP-reviews/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e328a512ee9b467694bd71fcf10b399e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab7d868be5b4b1cb5f4310270f611a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "437623be6ff34368984a2ee72bd2dc12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ae5a84fb504097ad316d491ecbe9a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f021897f823b4780aae1e59449914c2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cdf860a3ed2436abd5aae41b3054f44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d93ff8fe8ffe485f9e4639590a64587e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfc6b08381c34e038f4757076792d043"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288946e28d0d478db59233688e9bdfce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a13b02b8922f43ddba701717c0b86f8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c871ebfd84b4f34b506a7b85b74f259"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"logs\", name=\"dev\")\n",
    "bow_model = BoWModel(num_classes=5)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10, \n",
    "    callbacks=[L.pytorch.callbacks.EarlyStopping(monitor='val_loss', patience=7, strict=False, verbose=False, mode='min')],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(bow_model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T20:36:12.074600Z",
     "start_time": "2024-06-15T20:35:43.238057Z"
    }
   },
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
